{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672c9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tgt\n",
    "import sys\n",
    "import pandas as pd\n",
    "from re import match, sub, findall, finditer\n",
    "\n",
    "\n",
    "root_dir = '../'\n",
    "word_level_timing = root_dir+'word_level_timings'\n",
    "motion_label = root_dir+'motion_labels' \n",
    "original_annotation = root_dir + 'transcriptions_annotations'\n",
    "# word_level     = root_dir + 'word_level'\n",
    "# video          = root_dir + 'video' \n",
    "# audio          = root_dir +'audio' \n",
    "# documents      = root_dir + 'documents', \n",
    "# metadata       = root_dir+ 'metadata', \n",
    "# transcriptions = root_dir+'transcriptions_annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a3619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../transcriptions_annotations/r19/r19.TextGrid', '../transcriptions_annotations/r18/r18.TextGrid', '../transcriptions_annotations/r13/r13.TextGrid', '../transcriptions_annotations/r9/r9.TextGrid', '../transcriptions_annotations/r10/r10.TextGrid', '../transcriptions_annotations/r1/r1.TextGrid', '../transcriptions_annotations/r4/r4.TextGrid', '../transcriptions_annotations/r15/r15.TextGrid', '../transcriptions_annotations/r7/r7.TextGrid', '../transcriptions_annotations/r5/r5.TextGrid', '../transcriptions_annotations/r17/r17.TextGrid', '../transcriptions_annotations/r12/r12.TextGrid', '../transcriptions_annotations/r2/r2.TextGrid', '../transcriptions_annotations/r3/r3.TextGrid', '../transcriptions_annotations/r6/r6.TextGrid', '../transcriptions_annotations/r14/r14.TextGrid', '../transcriptions_annotations/r16/r16.TextGrid', '../transcriptions_annotations/r11/r11.TextGrid', '../transcriptions_annotations/r8/r8.TextGrid']\n"
     ]
    }
   ],
   "source": [
    "def get_all_textgrid_files(path):\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".TextGrid\"):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "    return filenames\n",
    "annotations = get_all_textgrid_files(original_annotation)\n",
    "print(annotations)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ca7a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_utt(utt, literal=False):\n",
    "    if not literal:\n",
    "        #replace variants, partial and misspoken words with standard spelling\n",
    "        utt = sub(\"\"\"<[vpm]=\"(.+?)\">.+?</[vpm]>\"\"\", lambda m:m.group(1), utt)\n",
    "        #remove fillers like \"{F aehm}\" entirely\n",
    "        utt = sub(\"\"\"{.*?}\"\"\", \"\", utt)\n",
    "        \n",
    "        #TO DO: resolve complex replacements like \"(der + der) + die) Katze\"\n",
    "        \n",
    "    else:\n",
    "        #remove brackets from fillers, i.e. \"{F aehm}\" becomes \"aehm\"\n",
    "        utt = sub(\"\"\"{(.*?)}\"\"\",lambda m:m.group(1),utt)\n",
    "    #remove all remaining xml-style tags    \n",
    "    utt = sub(\"\"\"<.*?>\"\"\",\"\",utt)\n",
    "    #remove open tags at the end of an utterance (can be removed once problems with the TextGrids are fixed)\n",
    "    utt = sub(\"\"\"<.*$\"\"\",\"\",utt)\n",
    "    #remove all remaining punctuation and brackets\n",
    "    utt = sub(\"\"\"[\\.:;,\\(\\)\\+\\$]\"\"\",\"\",utt)\n",
    "    #remove whitespace at the beginning and end of an utterance\n",
    "    utt = utt.strip()\n",
    "    #replace any amount of whitespace with a single space\n",
    "    utt = sub(\"\"\"\\s+\"\"\",\" \",utt)\n",
    "    return utt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02b3384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Methods to consume textgrids and convert to the disfluency\n",
    "corpus style for consistency across different possible raw formats.\n",
    "\n",
    "This file is distributed as part of DUEL corpus.\n",
    "\"\"\"\n",
    "\n",
    "# corpus, start_time deleted as parameters\n",
    "# how to do the basic version? e rps and f\n",
    "def disfluency_tags(utt):\n",
    "    \"\"\"returns the list of tags for each word (simply defined by split)\n",
    "    and also the list of tags for boundaries (one more than the utt length) \n",
    "    for repair points and laughter bouts. NB problem is: the laughter bout itself is a word\n",
    "    may in fact instead need to do this after we establish which words are proper words\"\"\"\n",
    "    utt = utt.split()\n",
    "    labels = [\"\",] * len(utt)\n",
    "    boundaries = [\"\",] * (len(utt)+1) # where do we use this?\n",
    "    inReparandum = 0\n",
    "    inRepair = 0\n",
    "    inFP = False # why does this start with True, changed to False\n",
    "    inLS = False\n",
    "    for i in range(0,len(utt)):\n",
    "        word = utt[i]\n",
    "        if i == 6: print(word)\n",
    "        if i == 8 : print(word, labels)\n",
    "        word_clean = clean_utt(word) # this is added\n",
    "        if word_clean == \"-\": # this was \"-\"\n",
    "            continue\n",
    "        if \"<laughter>\" in word or \"<laughter/>\" in word:\n",
    "            inLS = True\n",
    "        if \"<p\" in word:\n",
    "            labels[i] = \"<p/>\"\n",
    "        for j in range(0,len(word)):\n",
    "            filled_pause_begin = False\n",
    "            c = word[j]\n",
    "            if c==\"(\":\n",
    "                inReparandum+=1\n",
    "            if c == \"{\":\n",
    "                if j == len(word)-1:\n",
    "                    pass #edit term (non-fp)\n",
    "                elif word[j+1] == \"F\":\n",
    "                    inFP = True\n",
    "                    filled_pause_begin = True\n",
    "                else:\n",
    "                    pass\n",
    "        if i == 6: print(inLS, inFP, filled_pause_begin, inReparandum)\n",
    "        if i == 8: print(inLS, inFP, filled_pause_begin, inReparandum)   \n",
    "  \n",
    "        if inFP or filled_pause_begin: # using and instead of or removed all edit tags in {F Ahm\n",
    "            labels[i] = \"<e/>\"\n",
    "        elif inReparandum>0 and inFP==False:\n",
    "            labels[i] = \"<rm/>\"\n",
    "        elif inRepair>0 and inFP==False:\n",
    "            labels[i] = \"<rp/>\" # = instead of += for only one tag. however, open and close </rm> </rm> should be +=\n",
    "            \n",
    "        if inLS==True:\n",
    "            labels[i] = \"<ls/>\"\n",
    "        elif \"</laughter>\" in word:\n",
    "            inLS=False\n",
    "            \n",
    "        for j in range(0,len(word)):\n",
    "            c = word[j]\n",
    "            if c == \")\": inRepair-=1 # for now counting interegnum within the repairs\n",
    "            if c == \"+\": \n",
    "                inRepair += 1\n",
    "                inReparandum -= 1\n",
    "            if c ==\"}\": #out of the filled pause\n",
    "                inFP=False\n",
    "                \n",
    "        # fluent terms\n",
    "        if labels[i] == \"\":\n",
    "            labels[i] = \"<f/>\"               \n",
    "    #if inLS == True:\n",
    "    #    print \"WARNING NO LS END\", corpus, start_time\n",
    "        #raw_input()\n",
    "        \n",
    "        # labels[i-1] + utt[i] + labels[i]\n",
    "       # sandwiched_labels = labels[0] + utt + labels[1]\n",
    "    print(len(labels), len(utt))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e40b85ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<v='ein'>'n</v>\n",
      "False False False 0\n",
      "( ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '', '', '', '', '', '']\n",
      "False False False 1\n",
      "14 14\n",
      "['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<rm/>', '<rm/>', '<rm/>', '<rp/>', '<rp/>', '<f/>']\n"
     ]
    }
   ],
   "source": [
    "print(disfluency_tags(\"doch man kann ja noch mal <v='ein'>'n</v> paar ( <p='Durchgänge'>D-</p> + Durchgänge ) bauen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e35ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, f in enumerate(annotations):\n",
    "#     clear_text = list()\n",
    "#     textgrid = tgt.read_textgrid(f)\n",
    "#     tier_names = textgrid.get_tier_names()\n",
    "#     print(tier_names)\n",
    "#     for names in tier_names:\n",
    "#         text_tier = textgrid.get_tier_by_name(names)\n",
    "#         for annotation in text_tier.annotations:\n",
    "#             annotation.text = clean_utt(annotation.text)\n",
    "#     tgt.io.write_to_file(textgrid, './clean_text/'+str(f.split('/')[3].split('.')[0])+\".textgrid\")\n",
    "    # dataframe = pd.DataFrame(clear_text, columns=['file_name', 'tier_name', 'text', 'clean_text', 'start_time', 'end_time'])\n",
    "    # # print(f.split('/')[3].split('.')[0])\n",
    "    # dataframe.to_csv('./clean_text/'+str(f.split('/')[3].split('.')[0])+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9b4b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doch', 'man', 'kann', 'ja', 'noch', 'mal', \"<v='ein'>'n</v>\", 'paar', '(', \"<p='Durchgänge'>D-</p>\", '+', 'Durchgänge', ')', 'bauen']\n",
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "word clean doch\n",
      "stage two ['', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "word clean man\n",
      "stage two ['<f/>', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "word clean kann\n",
      "stage two ['<f/>', '<f/>', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "word clean ja\n",
      "stage two ['<f/>', '<f/>', '<f/>', '', '', '', '', '', '', '', '', '', '', '']\n",
      "word clean noch\n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '', '', '', '', '', '', '', '', '', '']\n",
      "word clean mal\n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '', '', '', '', '', '', '', '', '']\n",
      "word clean 'n\n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '', '', '', '', '', '', '', '']\n",
      "word clean paar\n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '', '', '', '', '', '', '']\n",
      "word clean \n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '', '', '', '', '', '']\n",
      "word clean D-\n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<rm/>', '<p/>', '', '', '', '']\n",
      "word clean \n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<rm/>', '<rm/>', '', '', '', '']\n",
      "word clean Durchgänge\n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<rm/>', '<rm/>', '<rm/>', '', '', '']\n",
      "word clean \n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<rm/>', '<rm/>', '<rm/>', '<rp/>', '', '']\n",
      "word clean bauen\n",
      "stage two ['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<rm/>', '<rm/>', '<rm/>', '<rp/>', '<rp/>', '']\n",
      "['<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<f/>', '<rm/>', '<rm/>', '<rm/>', '<rp/>', '<rp/>', '<f/>']\n"
     ]
    }
   ],
   "source": [
    "# for index, f in enumerate(annotations):\n",
    "#     print(\"file name is \", f)\n",
    "#     if index == 1:\n",
    "#         break\n",
    "#     clear_text = list()\n",
    "#     textgrid = tgt.read_textgrid(f)\n",
    "#     tier_names = textgrid.get_tier_names()\n",
    "#     print(\"Tier names are\", tier_names)\n",
    "#     for names in tier_names:\n",
    "#         text_tier = textgrid.get_tier_by_name(names)\n",
    "#         for annotation in text_tier.annotations:\n",
    "#             # print(annotation)\n",
    "#             if len(annotation.text) > 1:\n",
    "#                 print(annotation.text, annotation.start_time, annotation.end_time, disfluency_tags(annotation.text))\n",
    "#             # print(annotation.text, convert_to_disfluency_word_tag_tuples_from_raw(annotation.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b931148d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
