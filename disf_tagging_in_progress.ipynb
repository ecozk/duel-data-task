{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa89570-4ab6-42a6-9062-242351edc9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "import tgt\n",
    "from copy import deepcopy\n",
    "from re import match, sub, findall, finditer\n",
    "import glob\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca83b1a-21f7-4843-9901-4dd6cc3edcc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = './'\n",
    "word_level_timing = root_dir + 'word_level_timing'\n",
    "motion_label = root_dir + 'motion_labels' \n",
    "original_annotation = root_dir + 'transcriptions_annotations'\n",
    "lang = 'de'\n",
    "target_dir = \"./DUEL/{}\".format(lang)\n",
    "german_tagger = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7046dac-ca25-48d3-83e7-f8cce4e19e99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./word_level_timing/r1A_wordlevel.TextGrid', './word_level_timing/r13A_wordlevel.TextGrid', './word_level_timing/r3A_wordlevel.TextGrid', './word_level_timing/r19B_wordlevel.TextGrid', './word_level_timing/r15B_wordlevel.TextGrid', './word_level_timing/r11B_wordlevel.TextGrid', './word_level_timing/r19A_wordlevel.TextGrid', './word_level_timing/r2B_wordlevel.TextGrid', './word_level_timing/r3B_wordlevel.TextGrid', './word_level_timing/r12B_wordlevel.TextGrid', './word_level_timing/r9A_wordlevel.TextGrid', './word_level_timing/r10A_wordlevel.TextGrid', './word_level_timing/r9B_wordlevel.TextGrid', './word_level_timing/r18A_wordlevel.TextGrid', './word_level_timing/r17B_wordlevel.TextGrid', './word_level_timing/r8B_wordlevel.TextGrid', './word_level_timing/r15A_wordlevel.TextGrid', './word_level_timing/r5A_wordlevel.TextGrid', './word_level_timing/r12A_wordlevel.TextGrid', './word_level_timing/r7B_wodlevel.TextGrid', './word_level_timing/r6B_wordleve.TextGrid', './word_level_timing/r5B_wordlevel.TextGrid', './word_level_timing/r17A_wordlevel.TextGrid', './word_level_timing/r8A_wordlevel.TextGrid', './word_level_timing/r18B_wordlevel.TextGrid', './word_level_timing/r2A_wordlevel.TextGrid', './word_level_timing/r16B_wordlevel.TextGrid', './word_level_timing/r10B_wordlevel.TextGrid', './word_level_timing/r1B_wordlevel.TextGrid', './word_level_timing/r11A_wordlevel.TextGrid', './word_level_timing/r4A_wordlevel.TextGrid', './word_level_timing/r6A_wordlevel.TextGrid', './word_level_timing/r14B_wordlevel.TextGrid', './word_level_timing/r16A_wordlevel.TextGrid', './word_level_timing/r4B_wordlevel.TextGrid', './word_level_timing/r7A_wordlevel.TextGrid', './word_level_timing/r13B_wordlevel.TextGrid', './word_level_timing/r14A_wordlevel.TextGrid']\n"
     ]
    }
   ],
   "source": [
    "def get_all_textgrid_files(path):\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".TextGrid\"):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "    return filenames\n",
    "word_level_timing_annotation = get_all_textgrid_files(word_level_timing)\n",
    "print(word_level_timing_annotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba767561-91e2-477a-b175-f7a8f44259fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_index = {\n",
    "    1 : \"dream_apartment\",\n",
    "    2: \"film_script\",\n",
    "    3: \"border_control\"\n",
    "             }\n",
    "\n",
    "legal_tiers = {\"A-utts\" : [u\"A\", u\"A-utts;\"], \n",
    "               \"B-utts\" : [u\"B\", u\"B-utts;\", u\"B_utts\"], \n",
    "               \"A-turns\" : [u\"A-turns;\",\"A_turns\"], \n",
    "               \"B-turns\" : [ u\"B-turns;\",u\"B_turns\", u\"B-turns    \"],\n",
    "               \"A-laughter\" : [], \n",
    "               \"B-laughter\" : [u\"Bâˆ’laughter\"],\n",
    "               \"A-en\" : [u\"A-eng\", u\"A-english\",\n",
    "                         u\"A-fr_en\", u\"A-fr-en\",\n",
    "                         u\"A-fr_en;\",u\"Translation A\",\n",
    "                         u\"translation A\", u\"A translation\", u\"A Translation\"], \n",
    "               \"B-en\" : [u\"B-eng\", u\"B-english\",\n",
    "                         u\"B-fr_en\", u\"B-fr_en;\",\n",
    "                         u\"B_fr-en\", u\"Translation B\", \n",
    "                         u\"translation B\", u\"B translation\",\n",
    "                         u\"B Translation\", u\"B-fr-en\"],\n",
    "               \"Comments\" : [u\"Comments & questions\",\n",
    "                             u\"comments\", u\"Problems\"], \n",
    "               \"Part\" : [u\"part\"], \n",
    "               \"O\" : [u\"E\"]\n",
    "              }\n",
    "\n",
    "c = Counter()\n",
    "missing_c = defaultdict(list)\n",
    "global_tag_count = Counter()\n",
    "log_file = open(\"{}_errors.log\".format(lang), \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b3a93ec-dc3e-4312-9abf-a155d2226595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRead textgrid function\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read textgrid function\n",
    "\"\"\"\n",
    "# simply : tg = tgt.read_textgrid(tg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9220aa7-a09e-4660-883a-67b36eeceb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_utt(utt, literal=False):\n",
    "    if not literal:\n",
    "        #replace variants, partial and misspoken words with standard spelling\n",
    "        utt = sub(\"\"\"<[vpm]=\"(.+?)\">.+?</[vpm]>\"\"\", lambda m:m.group(1), utt)\n",
    "        #remove fillers like \"{F aehm}\" entirely\n",
    "        utt = sub(\"\"\"{.*?}\"\"\", \"\", utt)\n",
    "        \n",
    "        #TO DO: resolve complex replacements like \"(der + der) + die) Katze\"\n",
    "        \n",
    "    else:\n",
    "        #remove brackets from fillers, i.e. \"{F aehm}\" becomes \"aehm\"\n",
    "        utt = sub(\"\"\"{(.*?)}\"\"\",lambda m:m.group(1),utt)\n",
    "    #remove all remaining xml-style tags    \n",
    "    utt = sub(\"\"\"<.*?>\"\"\",\"\",utt)\n",
    "    #remove open tags at the end of an utterance (can be removed once problems with the TextGrids are fixed)\n",
    "    utt = sub(\"\"\"<.*$\"\"\",\"\",utt)\n",
    "    #remove all remaining punctuation and brackets\n",
    "    utt = sub(\"\"\"[\\.:;,\\(\\)\\+\\$]\"\"\",\"\",utt)\n",
    "    #remove whitespace at the beginning and end of an utterance\n",
    "    utt = utt.strip()\n",
    "    #replace any amount of whitespace with a single space\n",
    "    utt = sub(\"\"\"\\s+\"\"\",\" \",utt)\n",
    "    return utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1532cc9c-06a6-4fd7-9370-ae0bde01672f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Methods to consume textgrids and convert to the disfluency\n",
    "corpus style for consistency across different possible raw formats.\n",
    "\n",
    "This file is distributed as part of DUEL corpus.\n",
    "\"\"\"\n",
    "\n",
    "# corpus, start_time deleted as parameters\n",
    "# how to do the basic version? e rps and f\n",
    "def disfluency_tags(utt):\n",
    "    \"\"\"returns the list of tags for each word (simply defined by split)\n",
    "    and also the list of tags for boundaries (one more than the utt length) \n",
    "    for repair points and laughter bouts. NB problem is: the laughter bout itself is a word\n",
    "    may in fact instead need to do this after we establish which words are proper words\"\"\"\n",
    "    utt = utt.split()\n",
    "    labels = [\"\",] * len(utt)\n",
    "    boundaries = [\"\",] * (len(utt)+1) # where do we use this?\n",
    "    inRepair = 0\n",
    "    inFP = False # why does this start with True, changed to False\n",
    "    inLS = False\n",
    "    for i in range(0,len(utt)):\n",
    "        word = utt[i]\n",
    "        word_clean = clean_utt(word) # this is added\n",
    "        if word_clean == \"-\": # this was \"-\"\n",
    "            continue\n",
    "        \n",
    "        '''if \"<laughter>\" in word or \"<laughter/>\" in word:\n",
    "            inLS = True'''\n",
    "    \n",
    "        if \"<p\" in word:\n",
    "            labels[i] = \"<f/>\"\n",
    "        for j in range(0,len(word)):\n",
    "            filled_pause_begin = False\n",
    "            c = word[j]\n",
    "            # if c==\"(\":\n",
    "                \n",
    "            if c == \"{\":\n",
    "                if j == len(word)-1:\n",
    "                    pass #edit term (non-fp)\n",
    "                elif word[j+1] == \"F\":\n",
    "                    inFP = True\n",
    "                    filled_pause_begin = True\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        # choose where to put these conditions\n",
    "        \n",
    "        if inFP or filled_pause_begin: # using and instead of or removed all edit tags in {F Ahm\n",
    "            labels[i] += \"<e/>\"\n",
    "            \n",
    "            \n",
    "        elif inRepair>0 and inFP==False:\n",
    "            labels[i] += \"<rps/>\" # = instead of += for only one tag. however, open and close </rm> </rm> should be +=\n",
    "\n",
    "        for j in range(0,len(word)):\n",
    "            c = word[j]\n",
    "            if c == \"+\": \n",
    "                inRepair += 1 # inRepair boolean but \n",
    "            if c == \")\": inRepair-=1 # for now counting interegnum within the repairs\n",
    "\n",
    "            if c ==\"}\": #out of the filled pause\n",
    "                inFP=False\n",
    "            if c ==\"{\":\n",
    "                inFP=True\n",
    "                \n",
    "\n",
    "        # fluent terms\n",
    "        if labels[i] == \"\":\n",
    "            labels[i] = \"<f/>\"               \n",
    "    #if inLS == True:\n",
    "    #    print \"WARNING NO LS END\", corpus, start_time\n",
    "        #raw_input()\n",
    "        \n",
    "        # labels[i-1] + utt[i] + labels[i]\n",
    "       # sandwiched_labels = labels[0] + utt + labels[1] \n",
    "       # zip(word, label) two lists of tuples\n",
    "        \n",
    "    return (zip(utt, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "274dc23b-0d37-40d0-ab8c-ffa591086efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def textgrid_to_dict(tgfile):\n",
    "    \"\"\"Returns a dict with the tier names as keys and a list of\n",
    "    intervals of (start_time, end_time, text) as values.\n",
    "\n",
    "    :param tgfile: path to textgrid file\"\"\"\n",
    "\n",
    "    textgrid = tgt.read_textgrid(textgrid_file_name)\n",
    "    \n",
    "    tgdict = dict()\n",
    "    for tiername in textgrid.get_tier_names():\n",
    "        tgdict[tiername] = []\n",
    "        for textinterval in textgrid.get_tier_by_name(tiername):\n",
    "            if textinterval.text != '<sil>':\n",
    "                tgdict[tiername].append((float(textinterval.start_time),\n",
    "                                         float(textinterval.end_time),\n",
    "                                         str(textinterval.text\n",
    "                                             .encode(\"utf-8\").decode(\"utf-8\"))))\n",
    "    return tgdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aacef611-5c06-4bc0-b874-a97e97e91c22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "label: <f/> word Mhm word_start_time 555.791958 pos tags PROPN\n",
      "label: <f/> word ja word_start_time 557.064417 pos tags ADV\n",
      "label: <f/> word sehr word_start_time 562.649333 pos tags ADV\n",
      "label: <f/> word ja word_start_time 564.572188 pos tags ADV\n",
      "label: <f/> word also word_start_time 565.862271 pos tags ADV\n",
      "label: <f/> word mehr word_start_time 567.680271 pos tags ADV\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m word, label \u001b[39m=\u001b[39m tagged_utt[i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m clean_word \u001b[39m=\u001b[39m clean_utt(word)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m pos_tag \u001b[39m=\u001b[39m german_tagger(clean_word)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m clean_word \u001b[39m==\u001b[39m word_annotation \u001b[39mand\u001b[39;00m utterance_start_time \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m word_start_time \u001b[39mand\u001b[39;00m word_end_time \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m utterance_end_time \u001b[39mand\u001b[39;00m isVisited[index]\u001b[39m==\u001b[39m\u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     isVisited[index] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/spacy/language.py:1042\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1041\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1042\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1044\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/spacy/pipeline/tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m--> 126\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[1;32m    127\u001b[0m \u001b[39mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m lengths \u001b[39m=\u001b[39m NUMPY_OPS\u001b[39m.\u001b[39masarray1i([\u001b[39mlen\u001b[39m(seq) \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m Xs])\n\u001b[1;32m     76\u001b[0m Xf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(Xs, pad\u001b[39m=\u001b[39mpad)\n\u001b[0;32m---> 77\u001b[0m Yf, get_dXf \u001b[39m=\u001b[39m layer(Xf, is_train)\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYs: ListXd) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ListXd:\n\u001b[1;32m     80\u001b[0m     dYf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(dYs, pad\u001b[39m=\u001b[39mpad)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m d_output \u001b[39m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m](X, is_train)\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(X, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m [X[i] \u001b[39m+\u001b[39m Y[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/thinc/layers/maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape2f(W, nO \u001b[39m*\u001b[39m nP, nI)\n\u001b[0;32m---> 52\u001b[0m Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mgemm(X, W, trans2\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     53\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape1f(b, nO \u001b[39m*\u001b[39m nP)\n\u001b[1;32m     54\u001b[0m Z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape3f(Y, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transcription_dir = original_annotation\n",
    "tgsdict = dict()\n",
    "for experiment_name in sorted(os.listdir(transcription_dir)):\n",
    "    if \".DS_Store\" in experiment_name:\n",
    "        continue\n",
    "    tgsdict[experiment_name] = []\n",
    "    session_no = experiment_name[1: len(experiment_name)]\n",
    "    print(experiment_name[-1]) # r1, r2, r3...\n",
    "        \n",
    "    textgrid_file_name = transcription_dir + os.sep + experiment_name + os.sep + experiment_name + \".TextGrid\" # original transcription for that particular session    \n",
    "    # read textgrids\n",
    "    textgrid_dict = textgrid_to_dict(textgrid_file_name)\n",
    "\n",
    "    for index, utts in enumerate(textgrid_dict['A-utts']): # it return a list containing tuple of three elememt like starttime, endtime, utterance\n",
    "        isVisited = [False]*len(textgrid_dict['A-utts'])\n",
    "        participant = session_no + 'a'\n",
    "        word_level_textgrid_file_name = './word_level_timing/' + experiment_name + 'A_wordlevel.TextGrid' # reading corresponding word level timimg file\n",
    "        word_level_textgrid= tgt.read_textgrid(word_level_textgrid_file_name)\n",
    "        tier_names = word_level_textgrid.get_tier_names()\n",
    "        \n",
    "        utterance_start_time = utts[0] \n",
    "        utterance_end_time = utts[1]\n",
    "        utterance = utts[2]\n",
    "        \n",
    "        tagged_utt = list(disfluency_tags(utterance))\n",
    "              \n",
    "        for names in tier_names:\n",
    "            if names == 'ORT-MAU':\n",
    "                text_tier =  word_level_textgrid.get_tier_by_name(names)\n",
    "                for annotation in text_tier.annotations:\n",
    "                    \n",
    "                    word_start_time = annotation.start_time\n",
    "                    \n",
    "                    word_end_time = annotation.end_time\n",
    "                    \n",
    "                    word_annotation = annotation.text\n",
    "\n",
    "                    for i in range(0,len(tagged_utt)):\n",
    "                        word, label = tagged_utt[i]\n",
    "                        clean_word = clean_utt(word)\n",
    "                        pos_tag = german_tagger(clean_word)\n",
    "                        if clean_word == word_annotation and utterance_start_time <= word_start_time and word_end_time <= utterance_end_time and isVisited[index]==False:\n",
    "                            isVisited[index] = True\n",
    "                            for token in pos_tag:\n",
    "                                print('label:', label, 'word', word_annotation, 'word_start_time', word_start_time, 'pos tags', token.pos_)\n",
    "                            \n",
    "                            \n",
    "                    #doc = german_tagger(annotation.text)\n",
    "                    #for token in doc:\n",
    "                        #text_tier.add_annotations(token.pos_)\n",
    "                \n",
    "               # for annotation in text_tier.annotations:\n",
    "                    \n",
    "                # for i in range(0,len(utt)):\n",
    "                       # if utts[i] == annotation.text:\n",
    "                        #    doc = german_tagger(annotation.text)\n",
    "                        #    annotation.text, doc\n",
    "    ,\n",
    "       # print(utts.start_time)\n",
    "       # utts = list(disfluency_tags(utts[2]))\n",
    "       # print(participant, utts)\n",
    "        \n",
    "       # utts[i] == words \n",
    "        \n",
    "       #     start_time, end_time, pos_tag, word, participant_id\n",
    "    for index, utts in enumerate(textgrid_dict['A-utts']): # it return a list containing tuple of three elememt like starttime, endtime, utterance\n",
    "        isVisited = [False]*len(textgrid_dict['A-utts'])\n",
    "        participant = session_no + 'b'\n",
    "        word_level_textgrid_file_name = './word_level_timing/' + experiment_name + 'B_wordlevel.TextGrid' # reading corresponding word level timimg file\n",
    "        word_level_textgrid= tgt.read_textgrid(word_level_textgrid_file_name)\n",
    "        tier_names = word_level_textgrid.get_tier_names()\n",
    "        \n",
    "        utterance_start_time = utts[0] \n",
    "        utterance_end_time = utts[1]\n",
    "        utterance = utts[2]\n",
    "        \n",
    "        tagged_utt = list(disfluency_tags(utterance))\n",
    "        for names in tier_names:\n",
    "            if names == 'ORT-MAU':\n",
    "                text_tier =  word_level_textgrid.get_tier_by_name(names)\n",
    "                for annotation in text_tier.annotations:\n",
    "                    \n",
    "                    word_start_time = annotation.start_time\n",
    "                    \n",
    "                    word_end_time = annotation.end_time\n",
    "                    \n",
    "                    word_annotation = annotation.text\n",
    "\n",
    "                    for i in range(0,len(tagged_utt)):\n",
    "                        word, label = tagged_utt[i]\n",
    "                        clean_word = clean_utt(word)\n",
    "                        pos_tag = german_tagger(clean_word)\n",
    "                        if clean_word == word_annotation and utterance_start_time <= word_start_time and word_end_time <= utterance_end_time and isVisited[index]==False:\n",
    "                            isVisited[index] = True\n",
    "                            for token in pos_tag:\n",
    "                                print('label:', label, 'word', word_annotation, 'word_start_time', word_start_time, 'pos tags', token.pos_)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # for clean_utts in texgrids\n",
    "    #    if start_time <= time and end_time \n",
    "    \n",
    "    # tgt.io.write_to_file(textgrid, './disf_tags/'+str(f.split('/')[3].split('.')[0])+\".textgrid\")\n",
    "\n",
    "    # for uttsB in textgrid_dict['B-utts']:\n",
    "         \n",
    "    tgsdict[experiment_name].append(textgrid_dict)\n",
    "    # print(textgrid_file_name) # ./transcriptions_annotations/r1/r1.TextGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "141f1f10-1bd3-450c-9d95-1b700d0b1525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: './transcriptions_annotations/annotation.csv/annotation.csv.TextGrid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m textgrid_file_name_target \u001b[39m=\u001b[39m target_dir \u001b[39m+\u001b[39m os\u001b[39m.\u001b[39msep \u001b[39m+\u001b[39m experiment_name \u001b[39m+\u001b[39m os\u001b[39m.\u001b[39msep \u001b[39m+\u001b[39m experiment_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.TextGrid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# read textgrids\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m textgrid_dict \u001b[39m=\u001b[39m textgrid_to_dict(textgrid_file_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, interval \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(textgrid_dict[\u001b[39m'\u001b[39m\u001b[39mA-utts\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     participant \u001b[39m=\u001b[39m session_no \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32m/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtextgrid_to_dict\u001b[39m(tgfile):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a dict with the tier names as keys and a list of\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    intervals of (start_time, end_time, text) as values.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m    :param tgfile: path to textgrid file\"\"\"\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     textgrid \u001b[39m=\u001b[39m tgt\u001b[39m.\u001b[39;49mread_textgrid(textgrid_file_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     tgdict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/phantom/repair/de/duel-data-task/disf_tagging_in_progress.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m tiername \u001b[39min\u001b[39;00m textgrid\u001b[39m.\u001b[39mget_tier_names():\n",
      "File \u001b[0;32m~/miniconda3/envs/pyt/lib/python3.11/site-packages/tgt/io3.py:41\u001b[0m, in \u001b[0;36mread_textgrid\u001b[0;34m(filename, encoding, include_empty_intervals)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_textgrid\u001b[39m(filename, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, include_empty_intervals\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Read a Praat TextGrid file and return a TextGrid object. \u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m    If include_empty_intervals is False (the default), empty intervals\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m    are excluded. If True, they are included. Empty intervals from specific\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    tiers can be also included by specifying tier names as a string (for one tier)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    or as a list.'''\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     42\u001b[0m         \u001b[39m# Read whole file into memory ignoring empty lines and lines consisting\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         \u001b[39m# solely of a single double quotes.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         stg \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     45\u001b[0m             \u001b[39mif\u001b[39;00m line\u001b[39m.\u001b[39mstrip() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m ((stg[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFile type = \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mooTextFile\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m stg[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mObject class = \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTextGrid\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m         \u001b[39mand\u001b[39;00m (stg[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFile type = \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mooTextFile short\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m stg[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTextGrid\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)):\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: './transcriptions_annotations/annotation.csv/annotation.csv.TextGrid'"
     ]
    }
   ],
   "source": [
    "# transcription_dir = original_annotation\n",
    "\n",
    "# tgsdict = dict()\n",
    "\n",
    "# for experiment_name in sorted(os.listdir(transcription_dir)):\n",
    "    \n",
    "#     if \".DS_Store\" in experiment_name:\n",
    "#         continue\n",
    "        \n",
    "#     tgsdict[experiment_name] = []\n",
    "#     session_no = experiment_name[1: len(experiment_name)]\n",
    "#     print(experiment_name[-1]) # r1, r2, r3...\n",
    "        \n",
    "#     textgrid_file_name = transcription_dir + os.sep + experiment_name + os.sep + experiment_name + \".TextGrid\"\n",
    "#     textgrid_file_name_target = target_dir + os.sep + experiment_name + os.sep + experiment_name + \".TextGrid\"\n",
    "    \n",
    "    \n",
    "#     # read textgrids\n",
    "#     textgrid_dict = textgrid_to_dict(textgrid_file_name)\n",
    "        \n",
    "#     for i, interval in enumerate(textgrid_dict['A-utts']):\n",
    "        \n",
    "#         participant = session_no + 'a'\n",
    "        \n",
    "#         word_level_textgrid_file_name = './word_level_timings/' + experiment_name + 'A_wordlevel.TextGrid'\n",
    "    \n",
    "#         word_level_textgrid= tgt.read_textgrid(word_level_textgrid_file_name)\n",
    "        \n",
    "#         tier_names = word_level_textgrid.get_tier_names()\n",
    "        \n",
    "#         # utts = list(disfluency_tags(utts[2]))\n",
    "#         print(interval)\n",
    "#         utterance_start_time = utts[1] \n",
    "#         utterance = utts[2]\n",
    "        \n",
    "#         tagged_utt = list(disfluency_tags(utterance))\n",
    "        \n",
    "#         for i in range(0,len(tagged_utt)):\n",
    "#             word, label = tagged_utt[i]\n",
    "#             # clean_word = \n",
    "#             print(label)\n",
    "    \n",
    "#         # print(utts[2])\n",
    "#         # utt = list(disfluency_tags(utts[2]))\n",
    "#         # print(participant, utt)\n",
    "#         # print(len(utt))\n",
    "\n",
    "#     for utts in textgrid_dict['B-utts']:\n",
    "#         participant = session_no + 'b'\n",
    "#         word_level_textgrid_file_name = experiment_name + 'B_wordlevel.TextGrid'\n",
    "        \n",
    "#         print(utts[2])\n",
    "#         utts = list(disfluency_tags(utts[2]))\n",
    "#         print(participant, utts)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     # for clean_utts in texgrids\n",
    "#     #    if start_time <= time and end_time \n",
    "    \n",
    "#     # tgt.io.write_to_file(textgrid, './disf_tags/'+str(f.split('/')[3].split('.')[0])+\".textgrid\")\n",
    "\n",
    "#     # for uttsB in textgrid_dict['B-utts']:\n",
    "         \n",
    "#     tgsdict[experiment_name].append(textgrid_dict)\n",
    "#     # print(textgrid_file_name) # ./transcriptions_annotations/r1/r1.TextGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8710c6-9dea-4a25-859a-09523a18fb14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'r2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tg \u001b[38;5;241m=\u001b[39m \u001b[43mtgsdict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m tg[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-utts\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'r2'"
     ]
    }
   ],
   "source": [
    "# tg = tgsdict['r2']\n",
    "# tg[0]['B-utts'][0][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
