{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efa89570-4ab6-42a6-9062-242351edc9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "import tgt\n",
    "from copy import deepcopy\n",
    "from re import match, sub, findall, finditer\n",
    "import glob\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bca83b1a-21f7-4843-9901-4dd6cc3edcc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = './'\n",
    "word_level_timing = root_dir + 'word_level_timing'\n",
    "motion_label = root_dir + 'motion_labels' \n",
    "original_annotation = root_dir + 'transcriptions_annotations'\n",
    "lang = 'de'\n",
    "target_dir = \"./DUEL/{}\".format(lang)\n",
    "german_tagger = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7046dac-ca25-48d3-83e7-f8cce4e19e99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./word_level_timing/r1A_wordlevel.TextGrid', './word_level_timing/r13A_wordlevel.TextGrid', './word_level_timing/r3A_wordlevel.TextGrid', './word_level_timing/r19B_wordlevel.TextGrid', './word_level_timing/r15B_wordlevel.TextGrid', './word_level_timing/r11B_wordlevel.TextGrid', './word_level_timing/r19A_wordlevel.TextGrid', './word_level_timing/r2B_wordlevel.TextGrid', './word_level_timing/r3B_wordlevel.TextGrid', './word_level_timing/r12B_wordlevel.TextGrid', './word_level_timing/r9A_wordlevel.TextGrid', './word_level_timing/r10A_wordlevel.TextGrid', './word_level_timing/r9B_wordlevel.TextGrid', './word_level_timing/r18A_wordlevel.TextGrid', './word_level_timing/r17B_wordlevel.TextGrid', './word_level_timing/r8B_wordlevel.TextGrid', './word_level_timing/r15A_wordlevel.TextGrid', './word_level_timing/r5A_wordlevel.TextGrid', './word_level_timing/r12A_wordlevel.TextGrid', './word_level_timing/r7B_wodlevel.TextGrid', './word_level_timing/r6B_wordleve.TextGrid', './word_level_timing/r5B_wordlevel.TextGrid', './word_level_timing/r17A_wordlevel.TextGrid', './word_level_timing/r8A_wordlevel.TextGrid', './word_level_timing/r18B_wordlevel.TextGrid', './word_level_timing/r2A_wordlevel.TextGrid', './word_level_timing/r16B_wordlevel.TextGrid', './word_level_timing/r10B_wordlevel.TextGrid', './word_level_timing/r1B_wordlevel.TextGrid', './word_level_timing/r11A_wordlevel.TextGrid', './word_level_timing/r4A_wordlevel.TextGrid', './word_level_timing/r6A_wordlevel.TextGrid', './word_level_timing/r14B_wordlevel.TextGrid', './word_level_timing/r16A_wordlevel.TextGrid', './word_level_timing/r4B_wordlevel.TextGrid', './word_level_timing/r7A_wordlevel.TextGrid', './word_level_timing/r13B_wordlevel.TextGrid', './word_level_timing/r14A_wordlevel.TextGrid']\n"
     ]
    }
   ],
   "source": [
    "def get_all_textgrid_files(path):\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".TextGrid\"):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "    return filenames\n",
    "word_level_timing_annotation = get_all_textgrid_files(word_level_timing)\n",
    "print(word_level_timing_annotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba767561-91e2-477a-b175-f7a8f44259fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_index = {\n",
    "    1 : \"dream_apartment\",\n",
    "    2: \"film_script\",\n",
    "    3: \"border_control\"\n",
    "             }\n",
    "\n",
    "legal_tiers = {\"A-utts\" : [u\"A\", u\"A-utts;\"], \n",
    "               \"B-utts\" : [u\"B\", u\"B-utts;\", u\"B_utts\"], \n",
    "               \"A-turns\" : [u\"A-turns;\",\"A_turns\"], \n",
    "               \"B-turns\" : [ u\"B-turns;\",u\"B_turns\", u\"B-turns    \"],\n",
    "               \"A-laughter\" : [], \n",
    "               \"B-laughter\" : [u\"B−laughter\"],\n",
    "               \"A-en\" : [u\"A-eng\", u\"A-english\",\n",
    "                         u\"A-fr_en\", u\"A-fr-en\",\n",
    "                         u\"A-fr_en;\",u\"Translation A\",\n",
    "                         u\"translation A\", u\"A translation\", u\"A Translation\"], \n",
    "               \"B-en\" : [u\"B-eng\", u\"B-english\",\n",
    "                         u\"B-fr_en\", u\"B-fr_en;\",\n",
    "                         u\"B_fr-en\", u\"Translation B\", \n",
    "                         u\"translation B\", u\"B translation\",\n",
    "                         u\"B Translation\", u\"B-fr-en\"],\n",
    "               \"Comments\" : [u\"Comments & questions\",\n",
    "                             u\"comments\", u\"Problems\"], \n",
    "               \"Part\" : [u\"part\"], \n",
    "               \"O\" : [u\"E\"]\n",
    "              }\n",
    "\n",
    "c = Counter()\n",
    "missing_c = defaultdict(list)\n",
    "global_tag_count = Counter()\n",
    "log_file = open(\"{}_errors.log\".format(lang), \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b3a93ec-dc3e-4312-9abf-a155d2226595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRead textgrid function\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read textgrid function\n",
    "\"\"\"\n",
    "# simply : tg = tgt.read_textgrid(tg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9220aa7-a09e-4660-883a-67b36eeceb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_utt(utt, literal=False):\n",
    "    if not literal:\n",
    "        #replace variants, partial and misspoken words with standard spelling\n",
    "        utt = sub(\"\"\"<[vpm]=\"(.+?)\">.+?</[vpm]>\"\"\", lambda m:m.group(1), utt)\n",
    "        #remove fillers like \"{F aehm}\" entirely\n",
    "        utt = sub(\"\"\"{.*?}\"\"\", \"\", utt)\n",
    "        \n",
    "        #TO DO: resolve complex replacements like \"(der + der) + die) Katze\"\n",
    "        \n",
    "    else:\n",
    "        #remove brackets from fillers, i.e. \"{F aehm}\" becomes \"aehm\"\n",
    "        utt = sub(\"\"\"{(.*?)}\"\"\",lambda m:m.group(1),utt)\n",
    "    #remove all remaining xml-style tags    \n",
    "    utt = sub(\"\"\"<.*?>\"\"\",\"\",utt)\n",
    "    #remove open tags at the end of an utterance (can be removed once problems with the TextGrids are fixed)\n",
    "    utt = sub(\"\"\"<.*$\"\"\",\"\",utt)\n",
    "    #remove all remaining punctuation and brackets\n",
    "    utt = sub(\"\"\"[\\.:;,\\(\\)\\+\\$]\"\"\",\"\",utt)\n",
    "    #remove whitespace at the beginning and end of an utterance\n",
    "    utt = utt.strip()\n",
    "    #replace any amount of whitespace with a single space\n",
    "    utt = sub(\"\"\"\\s+\"\"\",\" \",utt)\n",
    "    return utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1532cc9c-06a6-4fd7-9370-ae0bde01672f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Methods to consume textgrids and convert to the disfluency\n",
    "corpus style for consistency across different possible raw formats.\n",
    "\n",
    "This file is distributed as part of DUEL corpus.\n",
    "\"\"\"\n",
    "\n",
    "# corpus, start_time deleted as parameters\n",
    "# how to do the basic version? e rps and f\n",
    "def disfluency_tags(utt):\n",
    "    \"\"\"returns the list of tags for each word (simply defined by split)\n",
    "    and also the list of tags for boundaries (one more than the utt length) \n",
    "    for repair points and laughter bouts. NB problem is: the laughter bout itself is a word\n",
    "    may in fact instead need to do this after we establish which words are proper words\"\"\"\n",
    "    utt = utt.split()\n",
    "    labels = [\"\",] * len(utt)\n",
    "    boundaries = [\"\",] * (len(utt)+1) # where do we use this?\n",
    "    inRepair = 0\n",
    "    inFP = False # why does this start with True, changed to False\n",
    "    inLS = False\n",
    "    for i in range(0,len(utt)):\n",
    "        word = utt[i]\n",
    "        word_clean = clean_utt(word) # this is added\n",
    "        if word_clean == \"-\": # this was \"-\"\n",
    "            continue\n",
    "        \n",
    "        '''if \"<laughter>\" in word or \"<laughter/>\" in word:\n",
    "            inLS = True'''\n",
    "    \n",
    "        if \"<p\" in word:\n",
    "            labels[i] = \"<f/>\"\n",
    "        for j in range(0,len(word)):\n",
    "            filled_pause_begin = False\n",
    "            c = word[j]\n",
    "            # if c==\"(\":\n",
    "                \n",
    "            if c == \"{\":\n",
    "                if j == len(word)-1:\n",
    "                    pass #edit term (non-fp)\n",
    "                elif word[j+1] == \"F\":\n",
    "                    inFP = True\n",
    "                    filled_pause_begin = True\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "        # choose where to put these conditions\n",
    "        \n",
    "        if inFP or filled_pause_begin: # using and instead of or removed all edit tags in {F Ahm\n",
    "            labels[i] += \"<e/>\"\n",
    "            \n",
    "            \n",
    "        elif inRepair>0 and inFP==False:\n",
    "            labels[i] += \"<rps/>\" # = instead of += for only one tag. however, open and close </rm> </rm> should be +=\n",
    "\n",
    "        for j in range(0,len(word)):\n",
    "            c = word[j]\n",
    "            if c == \"+\": \n",
    "                inRepair += 1 # inRepair boolean but \n",
    "            if c == \")\": inRepair-=1 # for now counting interegnum within the repairs\n",
    "\n",
    "            if c ==\"}\": #out of the filled pause\n",
    "                inFP=False\n",
    "            if c ==\"{\":\n",
    "                inFP=True\n",
    "                \n",
    "\n",
    "        # fluent terms\n",
    "        if labels[i] == \"\":\n",
    "            labels[i] = \"<f/>\"               \n",
    "    #if inLS == True:\n",
    "    #    print \"WARNING NO LS END\", corpus, start_time\n",
    "        #raw_input()\n",
    "        \n",
    "        # labels[i-1] + utt[i] + labels[i]\n",
    "       # sandwiched_labels = labels[0] + utt + labels[1] \n",
    "       # zip(word, label) two lists of tuples\n",
    "        \n",
    "    return (zip(utt, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "274dc23b-0d37-40d0-ab8c-ffa591086efd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def textgrid_to_dict(tgfile):\n",
    "    \"\"\"Returns a dict with the tier names as keys and a list of\n",
    "    intervals of (start_time, end_time, text) as values.\n",
    "\n",
    "    :param tgfile: path to textgrid file\"\"\"\n",
    "\n",
    "    textgrid = tgt.read_textgrid(textgrid_file_name)\n",
    "    \n",
    "    tgdict = dict()\n",
    "    for tiername in textgrid.get_tier_names():\n",
    "        tgdict[tiername] = []\n",
    "        for textinterval in textgrid.get_tier_by_name(tiername):\n",
    "            if textinterval.text != '<sil>':\n",
    "                tgdict[tiername].append((float(textinterval.start_time),\n",
    "                                         float(textinterval.end_time),\n",
    "                                         str(textinterval.text\n",
    "                                             .encode(\"utf-8\").decode(\"utf-8\"))))\n",
    "    return tgdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aacef611-5c06-4bc0-b874-a97e97e91c22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment number  1\n",
      "Processing for A utt\n",
      "[('Mhm', '<f/>')] [False]\n",
      "[('ja', '<f/>')] [False]\n",
      "label: <f/> word ja word_start_time 557.064417 pos tags ADV\n",
      "[('<laughter>sehr', '<f/>'), ('richtig</laughter>', '<f/>')] [False, False]\n",
      "label: <f/> word sehr word_start_time 562.649333 pos tags ADV\n",
      "[('ja', '<f/>')] [False]\n",
      "label: <f/> word ja word_start_time 564.572188 pos tags ADV\n",
      "[('also', '<f/>')] [False]\n",
      "label: <f/> word also word_start_time 565.862271 pos tags ADV\n",
      "[('mehr', '<f/>')] [False]\n",
      "[('ja', '<f/>')] [False]\n",
      "label: <f/> word ja word_start_time 578.008792 pos tags ADV\n",
      "[('ja', '<f/>')] [False]\n",
      "label: <f/> word ja word_start_time 578.874417 pos tags ADV\n",
      "[('genau', '<f/>')] [False]\n",
      "label: <f/> word genau word_start_time 579.246625 pos tags ADV\n",
      "[('mittig', '<f/>')] [False]\n",
      "label: <f/> word mittig word_start_time 585.197604 pos tags ADV\n",
      "[('ja', '<f/>')] [False]\n",
      "[('ja', '<f/>')] [False]\n",
      "label: <f/> word ja word_start_time 587.588062 pos tags ADV\n",
      "[('direkt', '<f/>'), ('in', '<f/>'), ('der', '<f/>'), ('Küche', '<f/>'), ('steht', '<f/>')] [False, False, False, False, False]\n",
      "label: <f/> word direkt word_start_time 598.217375 pos tags ADV\n",
      "label: <f/> word in word_start_time 598.4645 pos tags ADP\n",
      "label: <f/> word der word_start_time 598.5345 pos tags DET\n",
      "label: <f/> word Küche word_start_time 598.6745 pos tags NOUN\n",
      "[('achso', '<f/>')] [False]\n",
      "label: <f/> word achso word_start_time 601.419396 pos tags VERB\n",
      "[('ja', '<f/>'), ('ich', '<f/>'), (\"<v='glaube'>glaub'</v>\", '<f/>'), ('auch', '<f/>')] [False, False, False, False]\n",
      "label: <f/> word ja word_start_time 604.657604 pos tags ADV\n",
      "label: <f/> word ich word_start_time 604.724729 pos tags PRON\n",
      "[('also', '<f/>'), ('(', '<f/>'), (\"<p='allein'>allei-</p>\", '<f/>'), ('+', '<f/>'), ('allein', '<rps/>'), (')', '<rps/>'), ('schon', '<f/>'), ('{F', '<e/>'), ('äh}', '<e/>'), ('für', '<f/>'), ('die', '<f/>'), ('Schuhe', '<f/>')] [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "label: <f/> word also word_start_time 605.446979 pos tags ADV\n",
      "label: <f/> word allei- word_start_time 605.744104 pos tags X\n",
      "label: <rps/> word allein word_start_time 606.064104 pos tags ADV\n",
      "label: <f/> word schon word_start_time 606.304104 pos tags ADV\n",
      "label: <f/> word für word_start_time 606.684104 pos tags ADP\n",
      "label: <f/> word die word_start_time 606.954104 pos tags DET\n",
      "[('ja', '<f/>')] [False]\n",
      "label: <f/> word ja word_start_time 609.195125 pos tags ADV\n",
      "[('<laughter/><laughterOffset/>', '<f/>')] [False]\n",
      "[('ja', '<f/>'), ('ja', '<f/>'), ('genau', '<f/>')] [False, False, False]\n",
      "label: <f/> word ja word_start_time 617.255729 pos tags ADV\n",
      "label: <f/> word ja word_start_time 617.255729 pos tags ADV\n",
      "[('ja', '<f/>'), ('ja', '<f/>'), ('genau', '<f/>')] [False, False, False]\n",
      "label: <f/> word ja word_start_time 618.043437 pos tags ADV\n",
      "label: <f/> word ja word_start_time 618.043437 pos tags ADV\n",
      "label: <f/> word genau word_start_time 618.570563 pos tags ADV\n",
      "[('ja', '<f/>')] [False]\n",
      "[('ja', '<f/>'), ('ja', '<f/>'), ('genau', '<f/>'), ('(', '<f/>'), ('wo', '<f/>'), ('+', '<f/>'), ('(', '<rps/>'), ('wo', '<rps/>'), ('+', '<rps/>'), ('wo', '<rps/>'), (')', '<rps/>'), (')', '<rps/>'), ('man', '<f/>'), ('das', '<f/>'), ('Essen', '<f/>'), ('so', '<f/>'), ('durchreicht', '<f/>'), ('so', '<f/>')] [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "label: <f/> word ja word_start_time 640.866208 pos tags ADV\n",
      "label: <f/> word ja word_start_time 640.866208 pos tags ADV\n",
      "label: <f/> word genau word_start_time 641.133333 pos tags ADV\n",
      "label: <f/> word wo word_start_time 641.423333 pos tags ADV\n",
      "label: <rps/> word wo word_start_time 641.423333 pos tags ADV\n",
      "label: <rps/> word wo word_start_time 641.423333 pos tags ADV\n",
      "label: <f/> word man word_start_time 642.023333 pos tags PRON\n",
      "label: <f/> word das word_start_time 642.483333 pos tags PRON\n",
      "label: <f/> word Essen word_start_time 642.678333 pos tags PROPN\n",
      "label: <f/> word so word_start_time 642.923333 pos tags ADV\n",
      "label: <f/> word so word_start_time 642.923333 pos tags ADV\n",
      "label: <f/> word durchreicht word_start_time 643.083333 pos tags NOUN\n",
      "[('ja', '<f/>'), ('ja', '<f/>')] [False, False]\n",
      "label: <f/> word ja word_start_time 643.835292 pos tags ADV\n",
      "label: <f/> word ja word_start_time 643.835292 pos tags ADV\n",
      "[('ja', '<f/>')] [False]\n",
      "[('nee', '<f/>')] [False]\n",
      "[('<laughter/><laughterOffset/>', '<f/>')] [False]\n",
      "[('(also', '<f/>'), ('<p>m-</p>+', '<f/>'), ('also)', '<rps/>'), ('den', '<f/>'), ('Flur', '<f/>'), ('(', '<f/>'), (\"<v='würde'>würd'</v>\", '<f/>'), ('ich', '<f/>'), ('ja', '<f/>'), (\"<p='schon'>sch-</p>\", '<f/>'), ('+', '<f/>'), (\"<v='würde'>würd'</v>\", '<rps/>'), ('ich', '<rps/>'), ('ja', '<rps/>'), ('schon', '<rps/>'), (')', '<rps/>'), ('mal', '<f/>'), ('sagen', '<f/>'), ('ist', '<f/>'), ('ja', '<f/>'), ('okay', '<f/>')] [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "label: <f/> word also word_start_time 659.644875 pos tags ADV\n",
      "label: <rps/> word also word_start_time 659.644875 pos tags ADV\n",
      "label: <f/> word m- word_start_time 659.922 pos tags NOUN\n",
      "label: <f/> word den word_start_time 660.152 pos tags DET\n",
      "label: <f/> word Flur word_start_time 660.342 pos tags NOUN\n",
      "label: <f/> word ich word_start_time 660.712 pos tags PRON\n",
      "label: <rps/> word ich word_start_time 660.712 pos tags PRON\n",
      "label: <f/> word ja word_start_time 660.772 pos tags ADV\n",
      "label: <rps/> word ja word_start_time 660.772 pos tags ADV\n",
      "label: <f/> word ja word_start_time 660.772 pos tags ADV\n",
      "label: <f/> word sch- word_start_time 660.872 pos tags X\n",
      "label: <rps/> word schon word_start_time 661.472 pos tags ADV\n",
      "label: <f/> word mal word_start_time 661.622 pos tags ADV\n",
      "label: <f/> word sagen word_start_time 661.807 pos tags VERB\n",
      "label: <f/> word ist word_start_time 662.112 pos tags AUX\n"
     ]
    }
   ],
   "source": [
    "transcription_dir = original_annotation\n",
    "tgsdict = dict()\n",
    "for experiment_name in sorted(os.listdir(transcription_dir)):\n",
    "    if \".DS_Store\" in experiment_name:\n",
    "        continue\n",
    "    tgsdict[experiment_name] = []\n",
    "    session_no = experiment_name[1: len(experiment_name)]\n",
    "    print(\"experiment number \", experiment_name[-1]) # r1, r2, r3...\n",
    "        \n",
    "    textgrid_file_name = transcription_dir + os.sep + experiment_name + os.sep + experiment_name + \".TextGrid\" # original transcription for that particular session    \n",
    "    # read textgrids\n",
    "    textgrid_dict = textgrid_to_dict(textgrid_file_name)\n",
    "    print(\"Processing for A utt\")\n",
    "    for index, utts in enumerate(textgrid_dict['A-utts']): # it return a list containing tuple of three elememt like (starttime, endtime, utterance)\n",
    "        \n",
    "        participant = session_no + 'a'\n",
    "        word_level_textgrid_file_name = './word_level_timing/' + experiment_name + 'A_wordlevel.TextGrid' # reading corresponding word level timimg file\n",
    "        word_level_textgrid= tgt.read_textgrid(word_level_textgrid_file_name)\n",
    "        tier_names = word_level_textgrid.get_tier_names()\n",
    "        \n",
    "        utterance_start_time = utts[0] \n",
    "        utterance_end_time = utts[1]\n",
    "        utterance = utts[2]\n",
    "        \n",
    "\n",
    "        tagged_utt = list(disfluency_tags(utterance))\n",
    "        isVisited = [False]*len(tagged_utt) # keeping track of which utt which has been visited or not. only not visited utt has to be compare\n",
    "        print(tagged_utt, isVisited)\n",
    "        for names in tier_names:\n",
    "            if names == 'ORT-MAU':\n",
    "                text_tier =  word_level_textgrid.get_tier_by_name(names)\n",
    "                for annotation in text_tier.annotations:\n",
    "                    word_start_time = annotation.start_time                  \n",
    "                    word_end_time = annotation.end_time\n",
    "                    word_annotation = annotation.text                    \n",
    "                    for i in range(0,len(tagged_utt)):\n",
    "                        word, label = tagged_utt[i]\n",
    "                        clean_word = clean_utt(word)\n",
    "                        pos_tag = german_tagger(clean_word)\n",
    "                        if clean_word == word_annotation and utts[0] <= word_start_time and word_end_time < utts[1] and isVisited[i] == False:\n",
    "                            isVisited[i]= True\n",
    "                            for token in pos_tag:\n",
    "                                print('label:', label, 'word', word_annotation, 'word_start_time', word_start_time, 'pos tags', token.pos_)\n",
    "                            \n",
    "                            \n",
    "                    #doc = german_tagger(annotation.text)\n",
    "                    #for token in doc:\n",
    "                        #text_tier.add_annotations(token.pos_)\n",
    "                \n",
    "               # for annotation in text_tier.annotations:\n",
    "                    \n",
    "                # for i in range(0,len(utt)):\n",
    "                       # if utts[i] == annotation.text:\n",
    "                        #    doc = german_tagger(annotation.text)\n",
    "                        #    annotation.text, doc\n",
    "    \n",
    "       # print(utts.start_time)\n",
    "       # utts = list(disfluency_tags(utts[2]))\n",
    "       # print(participant, utts)\n",
    "        \n",
    "       # utts[i] == words \n",
    "        \n",
    "       #     start_time, end_time, pos_tag, word, participant_id\n",
    "    break\n",
    "    print(\"Processing for B utts\")\n",
    "    for index, utts in enumerate(textgrid_dict['B-utts']): # it return a list containing tuple of three elememt like starttime, endtime, utterance\n",
    "        isVisited = [False]*len(textgrid_dict['B-utts'])\n",
    "        participant = session_no + 'b'\n",
    "        word_level_textgrid_file_name = './word_level_timing/' + experiment_name + 'B_wordlevel.TextGrid' # reading corresponding word level timimg file\n",
    "        word_level_textgrid= tgt.read_textgrid(word_level_textgrid_file_name)\n",
    "        tier_names = word_level_textgrid.get_tier_names()\n",
    "        \n",
    "        utterance_start_time = utts[0] \n",
    "        utterance_end_time = utts[1]\n",
    "        utterance = utts[2]\n",
    "        \n",
    "        tagged_utt = list(disfluency_tags(utterance))\n",
    "        for names in tier_names:\n",
    "            if names == 'ORT-MAU':\n",
    "                text_tier =  word_level_textgrid.get_tier_by_name(names)\n",
    "                for annotation in text_tier.annotations:\n",
    "                    word_start_time = annotation.start_time\n",
    "                    word_end_time = annotation.end_time\n",
    "                    word_annotation = annotation.text\n",
    "                    for i in range(0,len(tagged_utt)):\n",
    "                        word, label = tagged_utt[i]\n",
    "                        clean_word = clean_utt(word)\n",
    "                        pos_tag = german_tagger(clean_word)\n",
    "                        if clean_word == word_annotation and utterance_start_time <= word_start_time and word_end_time <= utterance_end_time and isVisited[index]==False:\n",
    "                            isVisited[index] = True\n",
    "                            for token in pos_tag:\n",
    "                                print('label:', label, 'word', word_annotation, 'word_start_time', word_start_time, 'pos tags', token.pos_)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # for clean_utts in texgrids\n",
    "    #    if start_time <= time and end_time \n",
    "    \n",
    "    # tgt.io.write_to_file(textgrid, './disf_tags/'+str(f.split('/')[3].split('.')[0])+\".textgrid\")\n",
    "\n",
    "    # for uttsB in textgrid_dict['B-utts']:\n",
    "         \n",
    "    tgsdict[experiment_name].append(textgrid_dict)\n",
    "    # print(textgrid_file_name) # ./transcriptions_annotations/r1/r1.TextGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "141f1f10-1bd3-450c-9d95-1b700d0b1525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transcription_dir = original_annotation\n",
    "\n",
    "# tgsdict = dict()\n",
    "\n",
    "# for experiment_name in sorted(os.listdir(transcription_dir)):\n",
    "    \n",
    "#     if \".DS_Store\" in experiment_name:\n",
    "#         continue\n",
    "        \n",
    "#     tgsdict[experiment_name] = []\n",
    "#     session_no = experiment_name[1: len(experiment_name)]\n",
    "#     print(experiment_name[-1]) # r1, r2, r3...\n",
    "        \n",
    "#     textgrid_file_name = transcription_dir + os.sep + experiment_name + os.sep + experiment_name + \".TextGrid\"\n",
    "#     textgrid_file_name_target = target_dir + os.sep + experiment_name + os.sep + experiment_name + \".TextGrid\"\n",
    "    \n",
    "    \n",
    "#     # read textgrids\n",
    "#     textgrid_dict = textgrid_to_dict(textgrid_file_name)\n",
    "        \n",
    "#     for i, interval in enumerate(textgrid_dict['A-utts']):\n",
    "        \n",
    "#         participant = session_no + 'a'\n",
    "        \n",
    "#         word_level_textgrid_file_name = './word_level_timings/' + experiment_name + 'A_wordlevel.TextGrid'\n",
    "    \n",
    "#         word_level_textgrid= tgt.read_textgrid(word_level_textgrid_file_name)\n",
    "        \n",
    "#         tier_names = word_level_textgrid.get_tier_names()\n",
    "        \n",
    "#         # utts = list(disfluency_tags(utts[2]))\n",
    "#         print(interval)\n",
    "#         utterance_start_time = utts[1] \n",
    "#         utterance = utts[2]\n",
    "        \n",
    "#         tagged_utt = list(disfluency_tags(utterance))\n",
    "        \n",
    "#         for i in range(0,len(tagged_utt)):\n",
    "#             word, label = tagged_utt[i]\n",
    "#             # clean_word = \n",
    "#             print(label)\n",
    "    \n",
    "#         # print(utts[2])\n",
    "#         # utt = list(disfluency_tags(utts[2]))\n",
    "#         # print(participant, utt)\n",
    "#         # print(len(utt))\n",
    "\n",
    "#     for utts in textgrid_dict['B-utts']:\n",
    "#         participant = session_no + 'b'\n",
    "#         word_level_textgrid_file_name = experiment_name + 'B_wordlevel.TextGrid'\n",
    "        \n",
    "#         print(utts[2])\n",
    "#         utts = list(disfluency_tags(utts[2]))\n",
    "#         print(participant, utts)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     # for clean_utts in texgrids\n",
    "#     #    if start_time <= time and end_time \n",
    "    \n",
    "#     # tgt.io.write_to_file(textgrid, './disf_tags/'+str(f.split('/')[3].split('.')[0])+\".textgrid\")\n",
    "\n",
    "#     # for uttsB in textgrid_dict['B-utts']:\n",
    "         \n",
    "#     tgsdict[experiment_name].append(textgrid_dict)\n",
    "#     # print(textgrid_file_name) # ./transcriptions_annotations/r1/r1.TextGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e8710c6-9dea-4a25-859a-09523a18fb14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tg = tgsdict['r2']\n",
    "# tg[0]['B-utts'][0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2e507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
