{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import textgrid\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "import torch\n",
    "root_dir = './'\n",
    "word_level_timing = root_dir+'word_level_timings'\n",
    "motion_label = root_dir+'motion_labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die DET\n",
      "Sonne NOUN\n",
      "scheint VERB\n",
      "am ADP\n",
      "Himmel NOUN\n",
      "und CCONJ\n",
      "die DET\n",
      "Vögel NOUN\n",
      "singen VERB\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "german_text = \"Die Sonne scheint am Himmel und die Vögel singen.\"\n",
    "german_tagger = spacy.load(\"de_core_news_sm\")\n",
    "doc = german_tagger(german_text)\n",
    "# Extract tokens and POS tags\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./word_level_timings/r1A_wordlevel.TextGrid\n"
     ]
    }
   ],
   "source": [
    "def get_all_textgrid_files(path):\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".TextGrid\"):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "    return filenames\n",
    "word_level_timing_annotation = get_all_textgrid_files(word_level_timing)\n",
    "print(word_level_timing_annotation[0])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "for files in word_level_timing_annotation:\n",
    "    for index , tg in enumerate(textgrid.TextGrid.fromFile(word_level_timing_annotation[0])):\n",
    "        for interval in tg:\n",
    "#             print(interval.minTime, interval.maxTime, interval.mark)\n",
    "            if len(interval.mark) > 1:\n",
    "                doc = german_tagger(interval.mark)\n",
    "                if len(doc) == 1:\n",
    "                    for token in doc:\n",
    "                        data.append((token.text, token.pos_, interval.minTime, interval.maxTime))\n",
    "                        # print(token.text, token.pos_, end=' ')\n",
    "                        # print(interval.minTime, interval.maxTime)\n",
    "dataframe = pd.DataFrame(data, columns=['text','pos_tag', 'mintime','maxtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()\n",
    "dataframe.to_csv('./word_timing_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to define a dataset class first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordLevelTiming(torch.utils.data.Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, emdbedding_diminput_shape):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # self.word_embedding = \n",
    "        # self.lstm1 = nn.LSTM(embedding_dim, hidden_dim)\n",
    "    def forward(self, x)\n",
    "        # embeds = self.word_embedding\n",
    "        # lstm_out, _ = self.lstm(embeds.view(len(x), 1, -1))\n",
    "        # tag_space = self.hidden2tag(lstm_out.view(len(x), -1))\n",
    "        # tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "def train(train_loader,val_loader, model, loss_function, optimizer, epochs, device):\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "          (type(model).__name__, type(optimizer).__name__,\n",
    "           optimizer.param_groups[0]['lr'], epochs, device))\n",
    "\n",
    "    history = {}\n",
    "    history['loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['acc'] = []\n",
    "    history['val_acc'] = []\n",
    "    start_time_sec = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model=model.to(device)\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        num_train_correct = 0\n",
    "        num_train_examples = 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = batch['image'].to(device)\n",
    "            target = batch['label'].to(device)\n",
    "\n",
    "            output = model(images.float())\n",
    "            loss = loss_function(output, target.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.data.item()*images.size(0)\n",
    "            num_train_correct  += (torch.max(output, 1)[1] == target).sum().item()\n",
    "            num_train_examples += images.shape[0]\n",
    "        train_acc =  num_train_correct / num_train_examples\n",
    "        train_loss  = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # --- EVALUATE ON VALIDATION SET -------------------------------------\n",
    "\n",
    "        model.eval()\n",
    "        val_loss       = 0.0\n",
    "        num_val_correct  = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            target = batch['label'].to(device)\n",
    "\n",
    "            output = model(images.float())\n",
    "            loss = loss_function(output, target.float())\n",
    "\n",
    "            val_loss         += loss.data.item()*images.size(0)\n",
    "            num_val_correct  += (torch.max(output, 1)[1] == target).sum().item()\n",
    "            num_val_examples += images.shape[0]\n",
    "        val_acc  = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        print('Epoch %3d/%3d, train loss: %5.2f, train acc: %5.2f, val loss: %5.2f, val acc: %5.2f' % \\\n",
    "              (epoch+1, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "         # END OF TRAINING LOOP\n",
    "    end_time_sec       = time.time()\n",
    "    total_time_sec     = end_time_sec - start_time_sec\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print()\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "\n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
